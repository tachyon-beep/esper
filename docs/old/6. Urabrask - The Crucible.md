# **Technical Specification: Urabrask, the Blueprint Crucible**

**Version:** 0.1a
**Status:** Drafting
**Date:** 23 June 2025
**Author:** John Morrissey, Gemini AI

## **1. Purpose & Scope**

**Urabrask** is the **Blueprint Crucible** of the Esper Morphogenetic Platform. It serves as the system's automated quality assurance and risk analysis engine. Its primary mandate is to take novel candidate `Blueprints` designed by the Karn architect and subject them to a rigorous gauntlet of stress tests, performance benchmarks, and stability trials.

Urabrask's function is not to store blueprints, but to evaluate them and generate a comprehensive **Weighted Evaluation Profile (WEP)**. This profile is used to calculate a quantitative `risk_score` and assign a qualitative `conf_level` ('High', 'Moderate', or 'Speculative'). Only blueprints that are certified by Urabrask can be added to the Urza library for deployment by Tamiyo.

### **1.1 Design Principles**

Instead of a full neural network, Urabrask's policy can be implemented using a more lightweight and interpretable model, such as a Gradient Boosted Decision Tree (e.g., XGBoost) or a Logistic Regression model.

How it Works:

Training Data: Urabrask's model would be trained on a dataset where each row is a historical blueprint evaluation.

* Features (X): The flattened Weighted Evaluation Profile (WEP). This includes dozens of metrics like max_latency, gradient_variance, power_consumption, adversarial_robustness_score, etc.
  * Target Label (y): A binary or categorical label indicating the blueprint's ultimate outcome. This data comes from two sources:
  * FieldReports from Tamiyo: This is the most valuable data. Did the blueprint lead to a FOSSILIZED (success=0), CULLED (failure=1), or ROLLED_BACK (catastrophic_failure=2) state?
  * Urabrask's own tests: Did the blueprint fail a basic stability or performance test within the crucible itself?

Model Training: Nightly, a new risk-scoring model is trained on this accumulated historical data.

Inference (Risk Scoring): When a new candidate blueprint from Karn completes its tests and has a WEP generated, this WEP is fed into the trained model. The model's output is a probability (e.g., P(failure)), which becomes the risk_score.

```python
# Simplified logic within Urabrask
class RiskScoringPolicy:
    def __init__(self, model_path):
        self.model = load_model(model_path) # Load trained XGBoost model

    def calculate_risk_score(self, wep: Dict) -> float:
        """Calculates risk score from the Weighted Evaluation Profile."""
        feature_vector = preprocess_wep_to_vector(wep)
        # The model predicts the probability of failure
        risk_score = self.model.predict_proba(feature_vector)[:, 1]
        return float(risk_score)

# Mapping to confidence level
def map_risk_to_confidence(risk_score: float) -> str:
    if risk_score < 0.1:
        return "High"
    elif risk_score < 0.4:
        return "Moderate"
    else:
        return "Speculative"
```

Benefits of this Approach:

Fulfills your "No new NN" goal: It's a much simpler model class.
Highly Interpretable: You can easily inspect the trained model to see which features (e.g., "high variance in the final layer") are the biggest predictors of failure. This builds trust and aids debugging.
Data-Driven: The risk assessment isn't based on human-written rules but learns directly from what actually causes failures in the real world.
Efficient: These models are very fast to train and require minimal resources compared to a large NN.

---

## **2.  Universal Metadata Schema**

Every `Blueprint` must be exported with a **Universal Metadata Schema** so Tamiyo can make informed decisions about fitness & risk. Urabrask generates blueprint metadata according to a Universal Schema, which is authoritatively defined in the Urza specification. This includes a detailed WEP, a calculated risk_score, and a final conf_level. This schema is defined in the Urza specification, which details the required fields and their meanings.  

---

## 3. Blueprint Evaluation Scope

The Urabrask crucible is responsible for the evaluation and certification of all blueprints that are candidates for inclusion in the Urza library. The initial and authoritative catalogue of these blueprints—including their schemas, metadata, and implementations—is defined and maintained in the **`Urza - The Library.md`** specification. Urabrask references this catalogue to perform its testing duties, ensuring that its evaluations are always based on the single source of truth for blueprint design.

---

## **4.  Lifecycle, Safety & Ops**

### **4.1 Confidence-Gated Deployment Flow**

```mermaid
graph LR
  A[Blueprint] --> B[Urabrask Evaluation]
  B --> C[Generate WEP (Detailed Metrics)]
  C --> D[Calculate risk_score (0.0-1.0)]
  D --> E{Map to conf_level}
  E -->|High| F[Automatic Deployment Path]
  E -->|Moderate| G[Hardware‑Specific Approval Path]
  E -->|Speculative| H[Manual Review + PvP Path]
```

The deployment path for any new blueprint is strictly gated by the `conf_level` assigned to it by Urabrask. This level is determined by a rigorous and transparent three-step process:

1. **WEP Generation:** First, a comprehensive **Weighted Evaluation Profile (WEP)** is generated. This is a detailed dictionary containing dozens of performance, stability, and resource benchmarks.
2. **Risk Score Calculation:** Next, a single, normalized `risk_score` is calculated from the WEP. This provides a quantitative measure of the blueprint's aggregate deployment risk.
3. **Confidence Level Mapping:** Finally, the numeric `risk_score` is mapped to a human-readable `conf_level` ('High', 'Moderate', or 'Speculative'). This `conf_level` is the final output used by Tamiyo's policy engine to make deployment decisions.

<!-- end list -->

* **High:** Blueprints achieving a "High" confidence level are cleared for automatic deployment after passing all automated checks.
* **Moderate:** These blueprints require additional, targeted hardware-specific validation to ensure they meet the latency and memory budgets of the intended device.
* **Speculative:** Blueprints tagged as "Speculative" must undergo manual review and may be subjected to further competitive "Player vs. Player" (PvP) testing in the crucible before being cleared for any form of deployment.

### **4.2 Hardware‑Aware Constraints**

```python
class Blueprint(BlueprintBase):
    def __init__(self, *args, 
                 hardware_compatibility: List[str] = ["CUDA", "CPU"],
                 neuromorphic_support: bool = False,
                 quantum_ready: bool = False,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.hardware_compatibility = hardware_compatibility  # e.g. ["TPUv4", "Loihi2"]
        self.neuromorphic_support = neuromorphic_support      # Supports spike encoding
        self.quantum_ready = quantum_ready                    # Compatible with Pennylane/Qiskit
        
    def validate_hardware(self, target: HardwareProfile) -> bool:
        """Check compatibility with neuromorphic/quantum targets"""
        if target.arch == "NEUROMORPHIC" and not self.neuromorphic_support:
            return False
        if target.arch == "QUANTUM" and not self.quantum_ready:
            return False
        return super().validate_hardware(target)
```

Each blueprint carries lower (min_tflops) and upper (max_mem_gb) hardware bounds. Tamiyo’s planner discards any graft whose silicon envelope does not overlap the target device, preventing an edge sensor hub from even seeing blueprints intended for an A100 and vice-versa.

### **4.3 Provenance & Audit Logging**

All blueprint CRUD operations append an immutable hash-chained record to Postgres (mirrored nightly to cold storage). Every graft event logs the previous and current hash, blueprint ID, author, timestamp, and WEP rating.

Each graft event logs:

```plaintext
(hash_prev  |  hash_curr  |  blueprint_id  |  author  |  timestamp  |  WEP)
```

### **4.4 Curator KPIs & Auto‑Retirement**

Within Urza, a nightly curator pipeline enforces key metrics. These KPIs ensure that the library maintains a healthy balance of performance, safety, and innovation. The full list can be found in section 6 of the detailed design for Urza.

---

## **5. Operational Safety Framework**

The risk profiles and `conf_level` ratings generated by Urabrask are critical inputs into the platform-wide **Operational Safety Framework**. This framework ensures that the deployment of any blueprint is gated by its evaluated risk. The framework's first layer is a deterministic "Shield" that can veto an adaptation based on the blueprint's `conf_level` before it begins.

The authoritative and detailed specification for this framework, including the implementation of the deterministic Shield within the Tamiyo controller, resides in the **`Tamiyo - The Controller.md`** design document.

---

## **6. Lifecycle & Curation**

New blueprints reach the library through an automated ingestion path orchestrated by `Karn`, the system's evolutionary architect. Using a generative model trained with reinforcement learning, Karn discovers and proposes novel blueprint architectures designed to improve upon existing solutions based on real-world performance feedback. These candidates are packaged with the standard metadata schema and submitted to the Urabrask crucible for rigorous evaluation.

Static analysis first checks graph integrity and licence compliance; hardware validation then benchmarks the blueprint across representative devices. Successful candidates enter the main catalogue tagged "Speculative" by default. Their `conf_level` is upgraded only after they clear the curator KPIs over multiple nightly cycles and hardware profiles. Conversely, blueprints that under-perform—falling below the 90% success line for three consecutive hardware generations or being out-scored by newer designs in more than 80% of tasks—are automatically retired to an archival tier. The curator also monitors our novelty metric; if diversity dips below 0.4`, Karn is instructed to inject “exotic” blueprints from less-explored branches of the design space, ensuring the library doesn’t converge prematurely on a local optimum.

---

## **7. Open Items (v1.0 todo)**

Edge-benchmarks for FlashAttention Lite vs MQA. We need a clean apples-to-apples run on a Jetson Orin Nano, capturing perplexity, latency and joules-per-token, so Tamiyo has solid priors when choosing attention variants on low-power probes.

End-to-end JSON-schema validation in CI. A lightweight pytest plugin will parse every blueprint stub on each pull request, guaranteeing the metadata contract stays intact as the library grows.

Risk-index population from fresh Crucible data. The current risk_level_idx numbers are seeded from historical runs; they must be refreshed once the new canary system starts producing live failure-rate metrics.
