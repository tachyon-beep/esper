# **Oona: Message Bus Subsystem**

**Version:** 1.1
**Status:** Final
**Date:** 29 June 2025
**Author:** John Morrissey, Gemini AI

## **1. Introduction**

### **1.1. Purpose & Scope**

This document provides the formal detailed design for **Oona**, the asynchronous message bus and event-driven backbone of the Esper Morphogenetic Platform. Oona's sole purpose is to provide a reliable, scalable, and decoupled communication layer that enables the ten specialized subsystems to interact without direct dependencies. It is the infrastructure that realizes the system's event-sourcing and auditable history principles.

The scope of this document includes the definition of Oona's logical topic architecture, message contracts, delivery guarantees, and data retention policies. It does not cover the implementation of producer or consumer logic within other subsystems (e.g., how Tamiyo processes a message), only the architecture of the transport layer itself.

### **1.2. Architectural Goals & Principles**

Oona's design is governed by the core architectural principles of the Esper platform:

* **Decoupling:** Oona must serve as a pure intermediary, ensuring no subsystem needs direct knowledge of another's location or implementation. A publisher sends a message to a topic, and a consumer reads from it, with Oona managing the entire exchange.
* **Reliability & Durability:** The system must guarantee message delivery according to configured policies. All critical system events must be persisted to form an immutable log, enabling system state reconstruction and robust auditing.
* **Scalability:** The architecture must scale from a lightweight single-node deployment in Phase 1 to a high-throughput, multi-tenant distributed system in Phase 2.
* **Observability:** The structure of messages and topics must inherently support a complete and transparent audit trail of all system actions and decisions.

## **2. System Architecture**

### **2.1. Core Responsibilities**

* **Message Routing:** Route messages from a single producer to one or more consumers using a topic-based, publish-subscribe pattern.
* **Message Persistence:** Persist all events to disk for a configured retention period, acting as the system's primary event log.
* **Delivery Guarantees:** Provide at-least-once delivery guarantees for all critical communication channels.
* **Schema Enforcement (Phase 2):** In a production environment, leverage a schema registry to ensure all messages published to a topic adhere to a strict, versioned contract.

### **2.2. Technology Stack (Phased Implementation)**

Oona's implementation follows the two-phase approach defined in the HLD.

* **Phase 1 (Single-Node MVP): Redis Streams**

  * **Rationale:** Redis is lightweight, extremely fast, and its Streams data type provides the necessary persistence, consumer group, and ordered-log semantics required for the MVP. It is simple to deploy and manage within a `docker-compose` environment.

* **Phase 2 (Distributed Production): Apache Pulsar**

  * **Rationale:** Pulsar is purpose-built for large-scale, mission-critical event streaming and provides key features essential for a production Esper deployment:
    * **Multi-Tenancy:** Natively isolates tenants, allowing multiple, concurrent training runs to share the same Oona cluster without interference.
    * **Tiered Storage:** Automatically offloads older topic data to cost-effective object storage (e.g., S3), allowing for virtually indefinite retention of the complete event log for audit and RL training purposes.
    * **Built-in Schema Registry:** Enforces data contract consistency across all producers and consumers, which is critical in a complex, multi-component system.

## **3. Topic Architecture**

The topic architecture is the logical blueprint of the Esper system's communication. Topics are organized hierarchically (`<plane>.<subsystem>.<event>`) to provide clarity and fine-grained access control.

The following table details the primary information flows mediated by Oona:

| Topic Name | Publisher(s) | Consumer(s) | Payload Data Model | Frequency / Purpose |
| --- | --- | --- | --- | --- |
| `telemetry.seed.health` | `KasminaLayer` | `Tamiyo`, `Nissa` | `LayerHealthReport` | **High Frequency.** End-of-epoch reports containing the consolidated health metrics for all seeds in a layer. |
| `control.kasmina.commands` | `Tamiyo` | `KasminaLayer` | `KasminaControlCommand` | **Low Frequency.** The primary command channel in **Phase 2** for high-importance commands from Tamiyo to a specific KasminaLayer, ensuring full decoupling in a distributed environment. In **Phase 1**, this flow may be implemented as a direct API call for simplicity, with the message bus topic reserved for future use. |
| `innovation.field_reports` | `Tamiyo` | `Karn`, `Simic` | `FieldReport` | **Low Frequency.** Provides real-world performance feedback on a blueprint's outcome (`FOSSILIZED`, `CULLED`) to train Karn and Simic. |
| `innovation.candidates.submitted` | `Karn` | `Urabrask` | `CandidateBlueprint` | **Medium Frequency.** Karn submits newly generated blueprint architectures to the testing crucible for evaluation. |
| `innovation.certification.results` | `Urabrask` | `Urza`, `Karn` | `CertifiedBlueprint` | **Medium Frequency.** Urabrask publishes the full WEP and risk assessment for a blueprint, which Urza stores and Karn learns from. |
| `system.events.epoch` | `Tolaria` | `Tamiyo`, `Nissa` | `EpochEvent` | **Low Frequency.** The master "heartbeat" signal that drives epoch-synchronized state updates across the system. |
| `system.events.rollback` | `Tolaria` | `Tamiyo`, `Nissa` | `RollbackEvent` | **Very Low Frequency.** Critical event signaling that a system-level rollback has been executed, used for alerting and state reconciliation. |

## **4. Message Contracts & Schemas**

To ensure perfect interoperability, all messages adhere to a strict contract.

* **Canonical Schemas:** The authoritative definitions for all message payloads are the Rust structs located in the shared `contracts` crate.

* **"Fat Envelope" Pattern:** Every message payload is wrapped in a standard envelope containing rich metadata for observability and debugging, as specified in the HLD [cite: 737-738].

    ```json
    {
      "envelope": {
        "event_id": "uuid-v4-string",
        "sender_id": "tamiyo-controller-instance-1",
        "timestamp": "2025-06-29T14:30:00Z",
        "trace_id": "trace-id-for-distributed-tracing"
      },
      "payload": {
        // Canonical Rust struct (e.g., LayerHealthReport) serialized to JSON
        "layer_id": 12,
        "health_metrics_by_seed": { ... }
      }
    }
    ```

## **5. Data Lifecycle & Retention Policies**

A polyglot persistence strategy is used, applying different retention policies to different types of data based on their value and frequency.

* **High-Frequency Telemetry (`telemetry.seed.health`):**

  * **Retention:** 72 hours on the message broker.
  * **Rationale:** This data is consumed and processed into an internal state representation by Tamiyo almost immediately. Nissa consumes it for real-time dashboards and persists long-term aggregates, so the raw event stream is not needed for long.

* **Critical Innovation Events (`innovation.*` topics):**

  * **Retention:** Indefinite (leveraging Pulsar's tiered storage in Phase 2).
  * **Rationale:** This stream of events—Field Reports, certification results—constitutes the experience dataset for training the `Karn` and `Tamiyo` neural policies[cite: 724]. It is the most valuable long-term data asset in the system and must be preserved for retraining and analysis.

* **Commands & System Events (`control.*`, `system.*`):**

  * **Retention:** 30 days.
  * **Rationale:** This data is crucial for short-term operational debugging and auditing but has less long-term value than the innovation event stream.

## **6. Integration Patterns**

Oona is not just a message pipe; its architecture enables two critical design patterns for the Esper platform.

* **Event Sourcing:** Oona functions as the primary event log for the entire system. By persisting all messages, it creates a complete, replayable, and immutable history of every significant state change. This is invaluable for debugging complex emergent behavior, auditing decisions, and bootstrapping new training environments for the RL agents.

* **Command Query Responsibility Segregation (CQRS):** The topic architecture naturally separates state-changing operations (Commands) from data-reading operations [Queries].

  * **Commands:** Low-frequency, high-importance messages like `KasminaControlCommand` are sent to specific `control.*` topics and are typically consumed by a single service instance.
  * **Queries / State Derivation:** System state is not queried from a database but is derived by consumers reading the event stream. For example, Tamiyo "queries" the state of the system by processing the stream of `LayerHealthReport` events to build its internal world model. This allows the read and write paths to be scaled and optimized independently.
