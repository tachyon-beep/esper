# **System Design Document: Tolaria, The Training Academy**

Version: 0.1a  
Status: Drafting  
Date: 23 June 2025  
Author: John Morrissey, Gemini AI

## **1. Overview and Purpose**

### **1.1. Role in the Esper Ecosystem**

**Tolaria** is the foundational training environment for the Esper Morphogenetic Platform. It is not a passive script but an intelligent, state-aware training academy responsible for orchestrating the entire learning process. Tolaria provides the structured, temporal context—the epochs and steps—within which the host model learns, and all other Esper components operate.

Its mandate is to:

* Manage the master training and validation loops.
* Load and serve data to the host model.
* Own and manage the core optimizer and learning rate schedulers.
* Provide the "heartbeat" for the Esper system by invoking the Tamiyo controller at the correct cadence.
* Dynamically rebuild the model's optimizer after a successful Kasmina graft operation.
* Check the `AdaptationSignal` object returned by `tamiyo.step()`. This object contains two critical flags: `optimizer_needs_rebuild` and `emergency_rollback_required`, which Tolaria uses to manage the model's state.
* Execute an emergency rollback by restoring a checkpoint if the `emergency_rollback_required` flag is true.

Tolaria is the master process, the "world" in which the host model lives and evolves. It provides the stability and structure required for the dynamic adaptations orchestrated by Tamiyo and executed by Kasmina.

## **2. Architectural Model: The Epoch Lifecycle**

## **2. Architectural Model: The Epoch Lifecycle**

The core logic of Tolaria is defined by its epoch lifecycle. This sequence of operations ensures that training, validation, and morphogenetic adaptation occur in a well-defined, predictable order.

```mermaid
graph TD
    subgraph Tolaria Epoch Lifecycle
        A[Start Epoch] --> B[Training Loop];
        B --> C{For each batch in training_dataloader};
        C --> D[1. Forward Pass<br>(Model processes data, Kasmina seeds monitor)];
        D --> E[2. Calculate Loss];
        E --> F[3. Backward Pass<br>(Calculate gradients)];
        F --> G[4. Optimizer Step<br>(Update weights)];
        G --> C;

        C -- End of Training Batches --> H[Validation Loop];
        H --> I{For each batch in validation_dataloader};
        I --> J[1. Forward Pass (Inference Mode)];
        J --> K[2. Aggregate Global Metrics<br>(val_accuracy, val_loss)];
        K --> I;

        I -- End of Validation Batches --> L[End of Epoch Hook];
        L --> M[1. Assemble SystemStatePacket<br>(global_metrics, hardware_context, etc.)];
        M --> N[2. Invoke Tamiyo Controller<br><b>adaptation_signal = tamiyo.step(SystemStatePacket)</b>];
        N --> O{3. Check for Emergency Rollback<br><b>adaptation_signal.emergency_rollback_required?</b>};
        O -- Yes --> P[Restore Checkpoint<br>(Model & Tamiyo state)];
        P --> R[End Epoch];
        O -- No --> Q{4. Check for Optimizer Rebuild<br><b>adaptation_signal.optimizer_needs_rebuild?</b>};
        Q -- Yes --> S[Rebuild Optimizer<br>with DynamicOptimizerManager];
        Q -- No --> R;
        S --> R;
    end
```

### **2.1. Lifecycle Stages Explained**

1. **Training Loop:** Tolaria iterates through the training dataset. During each forward pass, the `KasminaSeed` modules embedded in the model silently monitor their assigned chunks. Tolaria's optimizer updates the model weights as in a standard training regimen.
2. **Validation Loop:** After the training loop, Tolaria evaluates the model's performance on a held-out validation set. It computes the global performance metrics (e.g., validation\_accuracy, validation\_loss) that are critical inputs for Tamiyo's decision-making process.
3. **End-of-Epoch Hook:** This is the crucial integration point for the Esper system.
      * Tolaria gathers all global metrics into a `SystemStatePacket`.
      * It calls the `tamiyo.step()` method, ceding control to the morphogenetic controller and receiving an `AdaptationSignal` object in return.
      * It first checks the `emergency_rollback_required` flag. If true, it restores the last checkpoint and ends the epoch to ensure system stability.
      * If no rollback is needed, it then checks the `optimizer_needs_rebuild` flag. This flag is set by Tamiyo after a blueprint has been successfully integrated (`FOSSILIZED`).
4. **Dynamic Optimizer Rebuilding:** If the `optimizer_needs_rebuild` flag is set, Tolaria invokes its `DynamicOptimizerManager`. This component scans the newly modified model, identifies the new parameters from the grafted blueprint, and re-initializes the optimizer to include them in the training process. This is a critical step to ensure new capacity is actually trained.

## **3. Core Components and Data Structures**

### **3.1. The TolariaTrainer Class**

The central orchestrator, implemented as a class that encapsulates the entire training process.

class TolariaTrainer:  
    def **init**(self, model: nn.Module, tamiyo: TamiyoController, config: TolariaConfig):  
        self.model = model  
        self.tamiyo = tamiyo  
        self.config = config  
        self.train_loader, self.val_loader = self.setup_dataloaders()  
        self.optimizer_manager = DynamicOptimizerManager(model, config.optimizer_config)  
        self.optimizer = self.optimizer_manager.get_optimizer()  
        self.current_epoch = 0

    def train(self):  
        """Main entry point to start the training academy."""  
        for epoch in range(self.config.num_epochs):  
            self.current_epoch = epoch  
            self._run_train_epoch()  
            global_metrics = self._run_validation_epoch()


            # End-of-Epoch Hook  
            system_state = self._assemble_system_state(global_metrics)  
            adaptation_signal = self.tamiyo.step(system_state)

            if adaptation_signal.emergency_rollback_required:  
                print("Emergency rollback triggered. Restoring from checkpoint...")
                self.restore_from_checkpoint()
            else:
                self.create_checkpoint()  # Save the current state

            if adaptation_signal.optimizer_needs_rebuild:  
                self.optimizer = self.optimizer_manager.rebuild_optimizer()

    def _run_train_epoch(self):  
        # Standard training loop logic...  
        pass

    def _run_validation_epoch(self) -> dict:  
        # Standard validation loop logic...  
        pass

    def _assemble_system_state(self, metrics: dict) -> SystemStatePacket:  
        # Gathers metrics and context into the packet for Tamiyo  
        pass
    
    def create_checkpoint(self):
        """Saves the state of the model and the Tamiyo controller."""
        # Logic to save both model.state_dict() and tamiyo.save_state_dict()
        pass

    def restore_from_checkpoint(self):
        """Restores the state of the model and the Tamiyo controller."""
        # Logic to load both model.state_dict() and tamiyo.load_state_dict()
        pass

### **3.2. DynamicOptimizerManager**

This is a critical, non-standard component required for morphogenetic training. It is responsible for managing an optimizer whose parameter groups can change during the training run.

* **Responsibilities:**  
  * Initializes the optimizer with the model's starting parameters.  
  * Maintains a manifest of all parameters currently being trained.  
  * Provides a rebuild_optimizer method that scans the model for any new parameters (from a grafted blueprint) that are not in its manifest.  
  * Re-initializes the optimizer, preserving the state (e.g., momentum buffers in Adam) for existing parameters while creating new state for the new parameters.

### **3.3. TolariaConfig**

A unified configuration structure for the entire training run.

## **4. Integration Points**

Tolaria serves as the primary integration hub, connecting the static training world with the dynamic Esper components.

* **Tolaria → Tamiyo:**  
  * **Invocation:** At the end of each epoch, TolariaTrainer calls tamiyo.step(system_state).  
  * **Data Passed:** It provides the SystemStatePacket, containing validation_accuracy, validation_loss, current_epoch, and hardware context.  
  * **Signal Received:** It checks the optimizer_needs_rebuild boolean flag returned by Tamiyo to know when to update its optimizer.  
* **Tolaria → Model (with Kasmina):**  
  * Tolaria treats the host model as a standard nn.Module. It is unaware of the internal KasminaSeed operations during the forward/backward pass.  
  * The key interaction is indirect: when Tolaria's DynamicOptimizerManager scans the model for new parameters, it discovers the components that Kasmina has added to the graph.

## **5. Initialization Sequence**

Setting up a training run with Tolaria follows a strict order:

1. **Load Configurations:** Load the TolariaConfig, which contains the subordinate TamiyoConfig and KasminaConfig.  
2. **Instantiate Model:** Build the host neural network. During this process, the KasminaSeed modules are injected into the specified layers according to the KasminaConfig.  
3. **Instantiate Tamiyo:** Create the TamiyoController instance using its configuration. Each KasminaSeed in the model is registered with this Tamiyo instance, establishing the link for commands and telemetry.  
4. **Instantiate Tolaria:** Create the TolariaTrainer instance, passing it the fully constructed model and the Tamiyo controller.  
5. **Begin Training:** Call tolaria_trainer.train() to start the academy.
