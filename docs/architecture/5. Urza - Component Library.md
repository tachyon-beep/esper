# **Technical Specification: Urza, The Blueprint Library**

**Version:** 0.1a
**Status:** Drafting
**Date:** 23 June 2025
**Author:** John Morrissey, Gemini AI

## 1  Purpose & Scope

The library (Urza) is the single source of truth for every seed blueprint that Kasmina can instantiate, Tamiyo can route, and Karn can evolve. It spans the full spectrum—from sub-1 B-parameter “tiny” models that run on micro-controllers right up to >100 B-parameter hyperscale giants—across text, vision, audio, graph, time-series, diffusion and reinforcement-learning domains.

> Every entry in the library includes:
>
> * **Tag**: The canonical identifier for the blueprint.
> * **Benefit**: A summary of the blueprint's primary advantage.
> * **Sweet-spot**: The model sizes and hardware profiles where the blueprint is most effective.
> * **`conf_level`**: The confidence level ("High", "Moderate", or "Speculative") assigned by the Urabrask evaluation process.

> **Safety note** Primitives flagged with ◊ are considered *risky*; Tamiyo’s outer‑loop shield must enforce a guard‑rail before live deployment.

---

## 2  Universal Metadata Schema

Every `Blueprint` JSON stub **MUST** expose the following keys so Tamiyo and the Shield can reason about fitness & risk:

| Field | Type | Example | Notes |
|---|---|---|---|
| `signature` | str | `"FlashAttn2::CUDA-90"` | Globally unique identifier. |
| `benefit` | str | `"Reduces memory usage"` | High-level summary of the blueprint's advantage. |
| `sweet_spot` | str | `"LLMs > 7B"` | The ideal models and hardware for this blueprint. |
| `wep` | dict | `{"perf": 0.9, "stabil": 0.95}` | **Weighted Evaluation Profile:** The detailed dictionary of key metrics from Urabrask. |
| `risk_score` | float | `0.08` | A calculated score (0.0-1.0) from the `wep`, representing deployment risk. |
| `resources` | dict | `{"params":4.2e6}` | Peak resource demands. |
| `conf_level` | str | `"High"` | Human-readable confidence tag derived from `risk_score`. |

---

Here is the updated and re-evaluated Initial Seed Catalogue. Entries have been added, removed, and modified for improved accuracy and clarity, reflecting current machine learning best practices.

### 3.1 Feature / Convolutional Extraction

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Depthwise-Separable Conv | Reduces FLOPs by 8-9x compared to standard convolution. | Mobile GPUs, Edge ASICs | High |
| Inverted Residual (MobileNetV2) | Efficiently processes features with an expand-squeeze channel design. | Models < 100M params | High |
| Fused-MBConv | Combines expansion, depthwise conv, and projection into a single kernel for faster execution. | Server-class GPUs, TPUs | High |
| GhostNet Module | Generates more feature maps from cheaper linear operations, reducing computational cost. | Models < 50M params | Moderate |
| Squeeze-and-Excitation (SE) | Adds channel-wise attention to re-weight features, boosting performance with minimal overhead. | Any CNN architecture | High |
| ConvNeXt Block | Modernized ResNet block using techniques like depthwise conv and large kernels for SoTA performance. | Vision models > 50M params | High |

### 3.2 Attention & Positional Encoding

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Multi-Head Attention (Baseline) | Standard, robust implementation of self-attention. | Any transformer model | High |
| FlashAttention-2 | Fused attention kernel that reduces memory usage and latency, especially for long sequences. | Server GPUs (A100/H100) | High |
| Grouped-Query Attention (GQA) | Improves inference throughput by sharing key-value heads, balancing performance and quality. | LLMs > 7B and < 70B | High |
| Multi-Query Attention (MQA) | Maximizes inference speed with a single key-value head; best for highly constrained environments. | Models < 7B, edge devices | High |
| Sliding Window / Local Attention | O(n) complexity attention, suitable for very long sequences or high-resolution images. | Vision transformers, long-context LLMs | Moderate |
| Rotary Position Embedding (RoPE) | Injects relative positional information directly into the attention mechanism, improving long-range coherence. | LLMs of all sizes | High |

### 3.3 Parameter-Efficient Adaptation

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| LoRA / QLoRA | Reduces trainable parameters by up to 10,000x by injecting low-rank matrices into layers. | LLMs < 70B, Vision models | High |
| (IA)³ | Re-scales activations with learned vectors, offering high efficiency for a smaller parameter budget. | Classifiers, smaller models | High |
| Adapter Modules | Injects small, task-specific bottleneck layers between existing model layers. | Any transformer-based model | High |
| Prefix / Prompt Tuning | Prepends a small set of trainable tokens to the input sequence to steer model behavior. | Any sequence model | High |
| BitFit | Fine-tunes only the bias terms of a model, providing a highly parameter-efficient option. | Tiny LLMs, initial fine-tuning | Moderate |
| VeRA (Vector-based Random Proj.) | Freezes random matrices and learns only small scaling vectors, using even fewer parameters than LoRA. | Models > 7B | Moderate |

### 3.4 Dynamic Routing & MoE

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Switch Transformer Router ◊ | Sparsely routes tokens to the best-fit "expert" sub-network, increasing capacity with constant FLOPs. | Models ≥ 3B | Moderate |
| Soft MoE | Routes tokens to multiple experts with learned weights, creating a differentiable, non-sparse alternative. | Vision transformers | Moderate |
| MoE-Lite (2-4 experts) ◊ | A cheaper MoE variant with fewer experts, suitable for smaller models. | Models 0.5B - 3B | Moderate |
| Learned Pruning Gate | Dynamically skips computation for less important tokens or channels at runtime. | Any architecture | Moderate |
| ACT (Adaptive Computation Time) | Allows recurrent models to dynamically vary the number of computation steps for each input. | RNNs, Universal Transformers | Speculative |

### 3.5 Memory & Cache

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| PagedAttention | Manages the KV-Cache in non-contiguous memory blocks, eliminating waste and enabling larger batch sizes. | LLM inference servers | High |
| Chunked KV-Cache Manager | Evicts and re-computes key-value cache chunks to fit long contexts on memory-limited hardware. | Edge LLM deployment | Moderate |
| Telemetry Monitor | A non-invasive seed that streams gradient statistics, activation norms, and energy usage for analysis. | All architectures | High |
| Latency Governor | A simple probe that rejects adaptations predicted to violate a pre-defined latency budget. | Real-time systems | High |

### 3.6 Recurrent / State Space

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Mamba / S6 Block | A modern state-space model with a selection mechanism, offering linear time complexity and long-range dependency modeling. | Language, Genomics, Time-series | High |
| Lightweight GRU | A classic, efficient gated recurrent unit for sequence modeling. | Sensor networks, small-scale NLP | High |
| Temporal Conv Net (TCN) | Uses causal, dilated convolutions to achieve a large receptive field for sequence tasks. | Time-series forecasting | High |
| Conformer Block | Combines convolutions and self-attention to effectively model both local and global dependencies in audio. | Speech recognition, ASR | High |
| Neural ODE ◊ | Models continuous-time dynamics by learning the derivative function of a system's state. | Physics sims, irregular time-series | Speculative |

### 3.7 Graph / Geometric

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Graph Conv Network (GCN) | Aggregates features from neighboring nodes; effective for homophilous graphs. | Social networks, citation graphs | High |
| Graph Attention (GATv2) | Uses attention to assign different weights to neighbors, improving performance on heterophilous graphs. | Traffic networks, knowledge graphs | High |
| Equivariant Conv (E(n)) ◊ | Preserves rotational and translational symmetries, crucial for physics and molecular modeling. | Robotics, drug discovery | Moderate |
| Graphormer Block ◊ | A full transformer architecture adapted for graph-level prediction tasks. | Molecular property prediction | Speculative |

### 3.8 Diffusion & Generative

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| U-Net Residual Block | The standard backbone component for diffusion models, capturing multi-scale features. | Vision/audio diffusion | High |
| Cross-Attention Injector | Conditions the diffusion process on external inputs like text or class labels. | Text-to-image, text-to-audio | High |
| Latent Diffusion VAE | A pre-trained autoencoder that allows diffusion to occur in a compressed latent space, saving VRAM. | High-resolution image generation | High |
| Classifier-Free Guidance | Improves sample quality by jointly training conditional and unconditional diffusion models. | Any guided diffusion model | High |

### 3.9 Regularization & Robustness

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Stochastic Depth | Prevents overfitting in very deep networks by randomly dropping entire residual blocks during training. | Deep convolutional networks | High |
| Label Smoothing | Reduces model overconfidence by using soft targets instead of one-hot labels during classification. | Any classification task | High |
| DropBlock | A structured form of dropout that removes contiguous regions of a feature map, improving regularization for CNNs. | Convolutional networks | Moderate |
| Adv-Robust Patch ◊ | A specialized module trained to detect and mitigate adversarial attacks in real-time. | Safety-critical vision systems | Speculative |

### 3.10 Fusion / Multimodal

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Cross-Modal Alignment | Maps different modalities (e.g., text and image) into a shared representation space. | CLIP-style models | High |
| FiLM Layer | Fuses modalities by having one network predict the affine transformation parameters (scale/shift) for another. | Audio-visual, vision-language | High |
| Gated Multimodal Unit | Uses gating mechanisms to dynamically control the influence of each modality in the final representation. | Robotics, sensor fusion | Moderate |
| PPO-Lite | A lightweight implementation of on-policy reinforcement learning for control tasks. | Robotics < 100M params | Moderate |

### 3.11 Security, Privacy & Attestation

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| HE-Gated Linear ◊ | Allows for inference on homomorphically encrypted data for specific linear operations. | Medical devices, private finance | Speculative |
| Zero-Trust Attestation | Uses a hardware TPM to cryptographically prove that a specific model is running untampered on a device. | Edge mission control, secure inference | High |
| Differentially Private Aggregation | Adds calibrated noise during federated learning to provide formal privacy guarantees. | Cross-enterprise models | Moderate |
| Model Watermarking | Embeds a detectable signature into the model's parameters to protect intellectual property. | Commercial deployments | Moderate |

### 3.12 Neuromorphic & Quantum

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Spiking Conv Layer | An event-based convolutional layer for neuromorphic hardware, offering ultra-low power consumption. | Dynamic vision sensors (DVS) | Moderate |
| Temporal Spike Encoder | A non-trainable module that converts standard video frames or sensor data into spike trains for SNNs. | Real-time robotics | High |
| QNN Variational Block ◊ | A hybrid classical-quantum layer used in Variational Quantum Eigensolvers (VQE). | Drug discovery, materials science | Speculative |
| Quantum Attention ◊ | A theoretical model aiming for exponential speedup on specific attention-based tasks using QPUs. | Large-scale graph problems | Speculative |

---

## 4 Lifecycle, Safety & Ops

### 4.1 **Confidence-Gated Deployment Flow**

All seeds pass through the Urabrask system, where they are assigned a `conf_level` based on their final `risk_score`. This `conf_level` then determines the deployment path. In addition to the performance data, each seed is assigned a WEP (Weighted Empirical Performance) rating based on its stability/failure rate and severity. This rating influences how the seed is deployed and monitored in production.

### 4.2 Provenance & Audit Logging

All blueprint CRUD operations append an immutable hash-chained record to Postgres (mirrored nightly to cold storage). Every graft event logs the previous and current hash, blueprint ID, author, timestamp, and WEP rating.

Each graft event logs:

```plaintext
(hash_prev  |  hash_curr  |  blueprint_id  |  author  |  timestamp  |  WEP)
```

### 4.4 Curator KPIs & Auto‑Retirement

A nightly curator pipeline enforces the following KPIs:

* **Success Rate**: ≥X % pass rate across a number of consecutive germinations (user configured).
* **Performance**: ≥5 % improvement over the previous best in 80 % of tasks (user configured).
* **Diversity**: Maintain a novelty score above 0.25 to ensure the library remains diverse and does not converge on local minima.

---

## 5  Operational Safety Framework

Tamiyo’s runtime safety net is layered. First, a deterministic Shield enforces hard rules: if a blueprint carries a WEP tag below the host’s minimum, it is excluded from consideration. Second, within those guard-rails a Safe-RL policy lets Tamiyo experiment: she learns which seeds boost task fitness while staying inside the shield’s envelope. Finally, promotion follows a disciplined canary pipeline—simulation → shadow traffic → 1 % live rollout—so even an allowed graft has to survive real-world noise before controlling mission-critical traffic. If a fault is detected at any stage, the offending seed is snap-rolled-back, marked “moderate confidence-card”, and Karn is tasked with breeding a safer variant. This tri-layer approach means we gain exploratory freedom without gambling on core functionality.

1. **Shield First:** heuristic based veto on latency, energy, and any other constraints applied by the parent model.
2. **Safe‑RL Inner Loop:** Tamiyo learns to maximise Δ‑fitness subject to shield.
3. **Canary Pipeline:** sim → shadow‑traffic → 1 % live rollout. Rollback SLA: 150 ms (edge) / 2 s (cloud).

---

## 6  Lifecycle & Curation

New blueprints reach the library through an automated ingestion path: Karn randomly mutates existing seeds, proposes seed graphs, and packages them with the metadata schema. Static analysis checks graph integrity and licence compliance; hardware validation then benchmarks the seed across representative devices. Successful candidates enter the main catalogue tagged Speculative by default. Their WEP rating is upgraded only after they clear the curator KPIs over multiple nightly cycles and hardware profiles. Conversely, seeds that under-perform—falling below the 90 % success line for three consecutive hardware generations or being out-scored by newer designs in more than 80 % of tasks—are automatically retired to an archival tier.

In order to avoid premature convergence on local optima, the curator also monitors our novelty metric; if diversity dips below 0.25, Karn is instructed to inject “exotic” seeds from less-explored branches of the design space, ensuring the library doesn’t converge prematurely on local minima. If diversity dips below 0.25, Karn is instructed to inject “exotic” seeds from less-explored branches of the design space, ensuring the library doesn’t converge prematurely on a local optimum.

Urza’s curation pipeline enforces the following key metrics: genetic diversity (≥ 40 % unique graphs), success rate across a preset number of generations (≥ 90 %), and impact on training outcomes. Seeds that flunk these thresholds are downgraded or retired; Karn is nudged to explore fresh territory rather than over-optimising one architecture. Thresholds will be dynamically scaled to a normal distribution based on the library’s current state, but the initial values are:

| KPI                          | Threshold                              | Action                                   |
| ---------------------------- | -------------------------------------- | ---------------------------------------- |
| **Genetic Diversity**        | ≥ 40 % unique tensor graphs in library | Trigger “exotic seed” injection if below |
| **Success Rate (3 HW gens)** | ≥ 90 %                                 | Retire under‑performers                  |
| **Latency Budget Breach**    | < 1 % of live inferences               | Auto‑downgrade WEP to Moderate           |
| **Energy‑Per‑Token Spike**   | < 5 % over rolling 24 h                | Flag for manual review                   |
| **Tamiyo's Opinion**         | < 0.7 favourability                    | Valued but middling performance blueprints are preserved  |
| **Training Impact**          | ≥ 5 % improvement in reward            | Upgrade WEP to High if sustained          |

---

## 7  Open Items (v1.0 todo)

Edge-benchmarks for FlashAttention Lite vs MQA. We need a clean apples-to-apples run on a Jetson Orin Nano, capturing perplexity, latency and joules-per-token, so Tamiyo has solid priors when choosing attention variants on low-power probes.

End-to-end JSON-schema validation in CI. A lightweight pytest plugin will parse every blueprint stub on each pull request, guaranteeing the metadata contract stays intact as the library grows.

Risk-index population from fresh Urabrask data. The current risk_level_idx numbers are seeded from historical runs; they must be refreshed once the new canary system starts producing live failure-rate metrics.
