# **Technical Specification: Urabrask, the Blueprint Crucible**

**Version:** 0.1a
**Status:** Drafting
**Date:** 23 June 2025
**Author:** John Morrissey, Gemini AI

## **1. Purpose & Scope**

**Urabrask** is the **Blueprint Crucible** of the Esper Morphogenetic Platform. It serves as the system's automated quality assurance and risk analysis engine. Its primary mandate is to take novel candidate `Blueprints` designed by the Karn architect and subject them to a rigorous gauntlet of stress tests, performance benchmarks, and stability trials.

Urabrask's function is not to store blueprints, but to evaluate them and generate a comprehensive **Weighted Evaluation Profile (WEP)**. This profile is used to calculate a quantitative `risk_score` and assign a qualitative `conf_level` ('High', 'Moderate', or 'Speculative'). Only blueprints that are certified by Urabrask can be added to the Urza library for deployment by Tamiyo.

---

## **2.  Universal Metadata Schema**

Every `Blueprint` must be exported with a **Universal Metadata Schema** so Tamiyo can make informed decisions about fitness & risk. Urabrask generates blueprint metadata according to a Universal Schema, which is authoritatively defined in the Urza specification. This includes a detailed WEP, a calculated risk_score, and a final conf_level. This schema is defined in the Urza specification, which details the required fields and their meanings.  

---

## 3. Blueprint Evaluation Scope

The Urabrask crucible is responsible for the evaluation and certification of all blueprints that are candidates for inclusion in the Urza library. The initial and authoritative catalogue of these blueprints—including their schemas, metadata, and implementations—is defined and maintained in the **`Urza - The Library.md`** specification. Urabrask references this catalogue to perform its testing duties, ensuring that its evaluations are always based on the single source of truth for blueprint design.

---

## **4.  Lifecycle, Safety & Ops**

### **4.1 Confidence-Gated Deployment Flow**

```mermaid
graph LR
  A[Blueprint] --> B[Urabrask Evaluation]
  B --> C[Generate WEP (Detailed Metrics)]
  C --> D[Calculate risk_score (0.0-1.0)]
  D --> E{Map to conf_level}
  E -->|High| F[Automatic Deployment Path]
  E -->|Moderate| G[Hardware‑Specific Approval Path]
  E -->|Speculative| H[Manual Review + PvP Path]
```

The deployment path for any new blueprint is strictly gated by the `conf_level` assigned to it by Urabrask. This level is determined by a rigorous and transparent three-step process:

1. **WEP Generation:** First, a comprehensive **Weighted Evaluation Profile (WEP)** is generated. This is a detailed dictionary containing dozens of performance, stability, and resource benchmarks.
2. **Risk Score Calculation:** Next, a single, normalized `risk_score` is calculated from the WEP. This provides a quantitative measure of the blueprint's aggregate deployment risk.
3. **Confidence Level Mapping:** Finally, the numeric `risk_score` is mapped to a human-readable `conf_level` ('High', 'Moderate', or 'Speculative'). This `conf_level` is the final output used by Tamiyo's policy engine to make deployment decisions.

<!-- end list -->

* **High:** Blueprints achieving a "High" confidence level are cleared for automatic deployment after passing all automated checks.
* **Moderate:** These blueprints require additional, targeted hardware-specific validation to ensure they meet the latency and memory budgets of the intended device.
* **Speculative:** Blueprints tagged as "Speculative" must undergo manual review and may be subjected to further competitive "Player vs. Player" (PvP) testing in the crucible before being cleared for any form of deployment.

### **4.2 Hardware‑Aware Constraints**

```python
class Blueprint(BlueprintBase):
    def __init__(self, *args, 
                 hardware_compatibility: List[str] = ["CUDA", "CPU"],
                 neuromorphic_support: bool = False,
                 quantum_ready: bool = False,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.hardware_compatibility = hardware_compatibility  # e.g. ["TPUv4", "Loihi2"]
        self.neuromorphic_support = neuromorphic_support      # Supports spike encoding
        self.quantum_ready = quantum_ready                    # Compatible with Pennylane/Qiskit
        
    def validate_hardware(self, target: HardwareProfile) -> bool:
        """Check compatibility with neuromorphic/quantum targets"""
        if target.arch == "NEUROMORPHIC" and not self.neuromorphic_support:
            return False
        if target.arch == "QUANTUM" and not self.quantum_ready:
            return False
        return super().validate_hardware(target)
```

Each blueprint carries lower (min_tflops) and upper (max_mem_gb) hardware bounds. Tamiyo’s planner discards any graft whose silicon envelope does not overlap the target device, preventing an edge sensor hub from even seeing blueprints intended for an A100 and vice-versa.

### **4.3 Provenance & Audit Logging**

All blueprint CRUD operations append an immutable hash-chained record to Postgres (mirrored nightly to cold storage). Every graft event logs the previous and current hash, blueprint ID, author, timestamp, and WEP rating.

Each graft event logs:

```plaintext
(hash_prev  |  hash_curr  |  blueprint_id  |  author  |  timestamp  |  WEP)
```

### **4.4 Curator KPIs & Auto‑Retirement**

Within Urza, a nightly curator pipeline enforces key metrics. These KPIs ensure that the library maintains a healthy balance of performance, safety, and innovation. The full list can be found in section 6 of the detailed design for Urza.

---

## **5. Operational Safety Framework**

The risk profiles and `conf_level` ratings generated by Urabrask are critical inputs into the platform-wide **Operational Safety Framework**. This framework ensures that the deployment of any blueprint is gated by its evaluated risk. The framework's first layer is a deterministic "Shield" that can veto an adaptation based on the blueprint's `conf_level` before it begins.

The authoritative and detailed specification for this framework, including the implementation of the deterministic Shield within the Tamiyo controller, resides in the **`Tamiyo - The Controller.md`** design document.

---

## **6. Lifecycle & Curation**

New blueprints reach the library through an automated ingestion path orchestrated by `Karn`, the system's evolutionary architect. Using a generative model trained with reinforcement learning, Karn discovers and proposes novel blueprint architectures designed to improve upon existing solutions based on real-world performance feedback. These candidates are packaged with the standard metadata schema and submitted to the Urabrask crucible for rigorous evaluation.

Static analysis first checks graph integrity and licence compliance; hardware validation then benchmarks the blueprint across representative devices. Successful candidates enter the main catalogue tagged "Speculative" by default. Their `conf_level` is upgraded only after they clear the curator KPIs over multiple nightly cycles and hardware profiles. Conversely, blueprints that under-perform—falling below the 90% success line for three consecutive hardware generations or being out-scored by newer designs in more than 80% of tasks—are automatically retired to an archival tier. The curator also monitors our novelty metric; if diversity dips below 0.25, Karn is instructed to inject “exotic” blueprints from less-explored branches of the design space, ensuring the library doesn’t converge prematurely on a local optimum.

---

## **7. Open Items (v1.0 todo)**

Edge-benchmarks for FlashAttention Lite vs MQA. We need a clean apples-to-apples run on a Jetson Orin Nano, capturing perplexity, latency and joules-per-token, so Tamiyo has solid priors when choosing attention variants on low-power probes.

End-to-end JSON-schema validation in CI. A lightweight pytest plugin will parse every blueprint stub on each pull request, guaranteeing the metadata contract stays intact as the library grows.

Risk-index population from fresh Crucible data. The current risk_level_idx numbers are seeded from historical runs; they must be refreshed once the new canary system starts producing live failure-rate metrics.
