# **Technical Specification: Urza, The Blueprint Library**

**Version:** 0.1a
**Status:** Drafting
**Date:** 23 June 2025
**Author:** John Morrissey, Gemini AI

## 1  Purpose & Scope

The library (Urza) is the single source of truth for every blueprints blueprint that Kasmina can instantiate, Tamiyo can route, and Karn can evolve. It spans the full spectrum—from sub-1 B-parameter “tiny” models that run on micro-controllers right up to >100 B-parameter hyperscale giants—across text, vision, audio, graph, time-series, diffusion and reinforcement-learning domains.

Every entry in the library includes:

* **Tag**: The canonical identifier for the blueprint.
* **Benefit**: A summary of the blueprint's primary advantage.
* **Sweet-spot**: The model sizes and hardware profiles where the blueprint is most effective.
* **`conf_level`**: The confidence level ("High", "Moderate", or "Speculative") assigned by the Urabrask evaluation process.

**Safety note** Primitives flagged with ◊ are considered *risky*; Tamiyo’s outer‑loop shield must enforce a guard‑rail before live deployment.

---

## 2  Universal Metadata Schema

Every `Blueprint` contains the following metadata to allow Tamiyo and the other agents to make informed decisions.

The Urza Library defines and owns the single, authoritative Universal Metadata Schema for all blueprints with other components like Karn, Tamiyo, and Urabrask acting as "customers" of this schema. The tables provided illustrate which specific fields within Urza's schema are populated by each respective component during the blueprint's lifecycle. Urza uses this information to manage the lifecycle of blueprints, including their creation, modification, and retirement.

The Universal Metadata Schema is described below:

| Field | Type | Example | Notes |
|---|---|---|---|
| `blueprint_id` | GUID | `"FlashAttn2::CUDA-90"` | Globally unique identifier. |
| `parent` | GUID | `"Urza::ComponentLibrary"` | The parent component that owns this blueprint. |
| `children` | list | `["Urza::FlashAttention2", "Urza::FlashAttentionLite"]` | List of child blueprints derived from this one. |
| `novelty_score` | float | `0.25` | A measure of how novel this blueprint is compared to existing ones. |

Table 1. Metadata for Karn

| Field | Type | Example | Notes |
|---|---|---|---|
| `tamiyo_interest` | float | `0.85` | The interest level of Tamiyo in this blueprint, indicating how often it is used in adaptations. |
| `use_cases` | str | [Set of Flags] | The ideal models and hardware for this blueprint as determined by Tamiyo. |
| `rollback_flag` | bool | `true` | Indicates if this blueprint has forced an emergency rollback and should be archived. |

Table 2. Metadata for Tamiyo

| Field | Type | Example | Notes |
|---|---|---|---|
| `elo_score` | int | 2345 | The ELO score of the blueprint, representing its performance in competitive evaluations. |
| `last_battle` | datetime | `"2025-06-23T12:00:00Z"` | Timestamp of the last competitive evaluation. |
| `last_opponent` | GUID | `"Urza::SimpleModel"` | The opponent blueprint in the last evaluation. |
| `urubrask_interest` | float | `0.75` | Urabrask's interest level in this blueprint. |
| `risk_score` | float | `0.08` | A calculated score (0.0-1.0) from the `wep`, representing deployment risk. |
| `conf_level` | str | `"High"` | Human-readable confidence tag derived from `risk_score`. |
| `wep` | dict | `{"perf": 0.9, "stabil": 0.95}` | **Weighted Evaluation Profile:** The detailed dictionary of key metrics from Urabrask. |

Table 3. Metadata for Urubrask

| Field | Type | Example | Notes |
|---|---|---|---|
| `created_at` | datetime | `"2025-06-23T12:00:00Z"` | Timestamp of when the blueprint was created. |
| `updated_at` | datetime | `"2025-06-23T12:00:00Z"` | Timestamp of the last update to the blueprint. |
| `lifecycle` | str | `"active"` | Current lifecycle state of the blueprint (e.g., "active", "archived"). |
| `author` | str | `"John Morrissey"` | The author of the blueprint. |
| `description` | str | `"FlashAttention-2 optimized for CUDA 9.0"` | A human-readable description of the blueprint. |
| `tags` | list | `["attention", "cuda", "high_performance"]` | Tags for categorization and searchability. |
| `hardware_profiles` | list | `["A100", "H100", "Jetson Orin"]` | List of hardware profiles where this blueprint is optimized to run. |

Table 4. Metadata for Urza

The various sub-systems will then use this metadata to make informed decisions about blueprint deployment and optimization. As part of the germintion and grafting process Tamiyo will note the success rate of the blueprint and the impact on the host model's performance. She will also generate flags for `use_cases` to highlight the ideal use cases for the blueprint. e.g. If it detects reduction in CPU usage, it will add a CPU_SAVING flag to the `use_cases` flags. If it notes a significant increase in throughput, it will add a THROUGHPUT_BOOST flag. If it detects significant improvements on a particular model profile (IMAGE, TEXT, AUDIO, etc.), it will add that MODEL_PROFILE flag.

## Initial Blueprint Catalogue

The following tables provide an initial catalogue of blueprints, organized by their primary function and benefit. Each blueprint is tagged with its `conf_level`, which indicates the confidence in its performance and stability based on Urabrask's evaluation.

### 3.1 Feature / Convolutional Extraction

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Depthwise-Separable Conv | Reduces FLOPs by 8-9x compared to standard convolution. | Mobile GPUs, Edge ASICs | High |
| Inverted Residual (MobileNetV2) | Efficiently processes features with an expand-squeeze channel design. | Models < 100M params | High |
| Fused-MBConv | Combines expansion, depthwise conv, and projection into a single kernel for faster execution. | Server-class GPUs, TPUs | High |
| GhostNet Module | Generates more feature maps from cheaper linear operations, reducing computational cost. | Models < 50M params | Moderate |
| Squeeze-and-Excitation (SE) | Adds channel-wise attention to re-weight features, boosting performance with minimal overhead. | Any CNN architecture | High |
| ConvNeXt Block | Modernized ResNet block using techniques like depthwise conv and large kernels for SoTA performance. | Vision models > 50M params | High |

### 3.2 Attention & Positional Encoding

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Multi-Head Attention (Baseline) | Standard, robust implementation of self-attention. | Any transformer model | High |
| FlashAttention-2 | Fused attention kernel that reduces memory usage and latency, especially for long sequences. | Server GPUs (A100/H100) | High |
| Grouped-Query Attention (GQA) | Improves inference throughput by sharing key-value heads, balancing performance and quality. | LLMs > 7B and < 70B | High |
| Multi-Query Attention (MQA) | Maximizes inference speed with a single key-value head; best for highly constrained environments. | Models < 7B, edge devices | High |
| Sliding Window / Local Attention | O(n) complexity attention, suitable for very long sequences or high-resolution images. | Vision transformers, long-context LLMs | Moderate |
| Rotary Position Embedding (RoPE) | Injects relative positional information directly into the attention mechanism, improving long-range coherence. | LLMs of all sizes | High |

### 3.3 Parameter-Efficient Adaptation

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| LoRA / QLoRA | Reduces trainable parameters by up to 10,000x by injecting low-rank matrices into layers. | LLMs < 70B, Vision models | High |
| (IA)³ | Re-scales activations with learned vectors, offering high efficiency for a smaller parameter budget. | Classifiers, smaller models | High |
| Adapter Modules | Injects small, task-specific bottleneck layers between existing model layers. | Any transformer-based model | High |
| Prefix / Prompt Tuning | Prepends a small set of trainable tokens to the input sequence to steer model behavior. | Any sequence model | High |
| BitFit | Fine-tunes only the bias terms of a model, providing a highly parameter-efficient option. | Tiny LLMs, initial fine-tuning | Moderate |
| VeRA (Vector-based Random Proj.) | Freezes random matrices and learns only small scaling vectors, using even fewer parameters than LoRA. | Models > 7B | Moderate |

### 3.4 Dynamic Routing & MoE

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Switch Transformer Router ◊ | Sparsely routes tokens to the best-fit "expert" sub-network, increasing capacity with constant FLOPs. | Models ≥ 3B | Moderate |
| Soft MoE | Routes tokens to multiple experts with learned weights, creating a differentiable, non-sparse alternative. | Vision transformers | Moderate |
| MoE-Lite (2-4 experts) ◊ | A cheaper MoE variant with fewer experts, suitable for smaller models. | Models 0.5B - 3B | Moderate |
| Learned Pruning Gate | Dynamically skips computation for less important tokens or channels at runtime. | Any architecture | Moderate |
| ACT (Adaptive Computation Time) | Allows recurrent models to dynamically vary the number of computation steps for each input. | RNNs, Universal Transformers | Speculative |

### 3.5 Memory & Cache

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| PagedAttention | Manages the KV-Cache in non-contiguous memory blocks, eliminating waste and enabling larger batch sizes. | LLM inference servers | High |
| Chunked KV-Cache Manager | Evicts and re-computes key-value cache chunks to fit long contexts on memory-limited hardware. | Edge LLM deployment | Moderate |
| Telemetry Monitor | A non-invasive blueprint that streams gradient statistics, activation norms, and energy usage for analysis. | All architectures | High |
| Latency Governor | A simple probe that rejects adaptations predicted to violate a pre-defined latency budget. | Real-time systems | High |

### 3.6 Recurrent / State Space

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Mamba / S6 Block | A modern state-space model with a selection mechanism, offering linear time complexity and long-range dependency modeling. | Language, Genomics, Time-series | High |
| Lightweight GRU | A classic, efficient gated recurrent unit for sequence modeling. | Sensor networks, small-scale NLP | High |
| Temporal Conv Net (TCN) | Uses causal, dilated convolutions to achieve a large receptive field for sequence tasks. | Time-series forecasting | High |
| Conformer Block | Combines convolutions and self-attention to effectively model both local and global dependencies in audio. | Speech recognition, ASR | High |
| Neural ODE ◊ | Models continuous-time dynamics by learning the derivative function of a system's state. | Physics sims, irregular time-series | Speculative |

### 3.7 Graph / Geometric

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Graph Conv Network (GCN) | Aggregates features from neighboring nodes; effective for homophilous graphs. | Social networks, citation graphs | High |
| Graph Attention (GATv2) | Uses attention to assign different weights to neighbors, improving performance on heterophilous graphs. | Traffic networks, knowledge graphs | High |
| Equivariant Conv (E(n)) ◊ | Preserves rotational and translational symmetries, crucial for physics and molecular modeling. | Robotics, drug discovery | Moderate |
| Graphormer Block ◊ | A full transformer architecture adapted for graph-level prediction tasks. | Molecular property prediction | Speculative |

### 3.8 Diffusion & Generative

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| U-Net Residual Block | The standard backbone component for diffusion models, capturing multi-scale features. | Vision/audio diffusion | High |
| Cross-Attention Injector | Conditions the diffusion process on external inputs like text or class labels. | Text-to-image, text-to-audio | High |
| Latent Diffusion VAE | A pre-trained autoencoder that allows diffusion to occur in a compressed latent space, saving VRAM. | High-resolution image generation | High |
| Classifier-Free Guidance | Improves sample quality by jointly training conditional and unconditional diffusion models. | Any guided diffusion model | High |

### 3.9 Regularization & Robustness

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Stochastic Depth | Prevents overfitting in very deep networks by randomly dropping entire residual blocks during training. | Deep convolutional networks | High |
| Label Smoothing | Reduces model overconfidence by using soft targets instead of one-hot labels during classification. | Any classification task | High |
| DropBlock | A structured form of dropout that removes contiguous regions of a feature map, improving regularization for CNNs. | Convolutional networks | Moderate |
| Adv-Robust Patch ◊ | A specialized module trained to detect and mitigate adversarial attacks in real-time. | Safety-critical vision systems | Speculative |

### 3.10 Fusion / Multimodal

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Cross-Modal Alignment | Maps different modalities (e.g., text and image) into a shared representation space. | CLIP-style models | High |
| FiLM Layer | Fuses modalities by having one network predict the affine transformation parameters (scale/shift) for another. | Audio-visual, vision-language | High |
| Gated Multimodal Unit | Uses gating mechanisms to dynamically control the influence of each modality in the final representation. | Robotics, sensor fusion | Moderate |
| PPO-Lite | A lightweight implementation of on-policy reinforcement learning for control tasks. | Robotics < 100M params | Moderate |

### 3.11 Security, Privacy & Attestation

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| HE-Gated Linear ◊ | Allows for inference on homomorphically encrypted data for specific linear operations. | Medical devices, private finance | Speculative |
| Zero-Trust Attestation | Uses a hardware TPM to cryptographically prove that a specific model is running untampered on a device. | Edge mission control, secure inference | High |
| Differentially Private Aggregation | Adds calibrated noise during federated learning to provide formal privacy guarantees. | Cross-enterprise models | Moderate |
| Model Watermarking | Embeds a detectable signature into the model's parameters to protect intellectual property. | Commercial deployments | Moderate |

### 3.12 Neuromorphic & Quantum

| Tag | Benefit | Sweet-spot | conf_level |
| --- | --- | --- | --- |
| Spiking Conv Layer | An event-based convolutional layer for neuromorphic hardware, offering ultra-low power consumption. | Dynamic vision sensors (DVS) | Moderate |
| Temporal Spike Encoder | A non-trainable module that converts standard video frames or sensor data into spike trains for SNNs. | Real-time robotics | High |
| QNN Variational Block ◊ | A hybrid classical-quantum layer used in Variational Quantum Eigensolvers (VQE). | Drug discovery, materials science | Speculative |
| Quantum Attention ◊ | A theoretical model aiming for exponential speedup on specific attention-based tasks using QPUs. | Large-scale graph problems | Speculative |

---

## 4 Lifecycle, Safety & Ops

### 4.1 **Confidence-Gated Deployment Flow**

All blueprints pass through the Urabrask system, where their Weighted Evaluation Profile (WEP) is generated based on performance, stability, and resource metrics. This WEP is then used to calculate a `risk_score`, which determines the `conf_level` of the blueprint. The `conf_level` and WEP are made available to Tamiyo to allows her to make informed decisions about which Blueprints to use in the host model.

### 4.2 Provenance & Audit Logging

All blueprint CRUD operations append an immutable hash-chained record to Postgres (mirrored nightly to cold storage). Every graft event logs the previous and current hash, blueprint ID, author, timestamp, and WEP rating.

Each graft event logs:

```plaintext
(hash_prev  |  hash_curr  |  blueprint_id  |  author  |  timestamp  |  WEP)
```

---

## **5. Operational Safety Framework**

The Esper platform's multi-layered approach to safety, which governs how blueprints from the Urza library are deployed, is centrally defined to ensure consistency. This framework begins with a deterministic "Shield" that enforces hard, pre-defined constraints on any proposed adaptation at runtime.

The authoritative and detailed specification for the complete **Operational Safety Framework**, including the implementation of the `SafetyValidator` Shield and the `SafetyConfig` profiles, resides in the **`Tamiyo - The Controller.md`** design document.

---

## **6. Lifecycle & Curation**

New blueprints reach the library through an automated ingestion path orchestrated by `Karn`, the system's evolutionary architect. Successful candidates enter the main catalogue tagged "Speculative" by default and are upgraded only after clearing curator KPIs. Conversely, blueprints that under-perform are automatically retired to an archival tier.

Urza’s curation pipeline enforces the following key metrics, which are exposed via its API for other components to monitor:

| KPI | Threshold | Action / Consequence |
|---|---|---|
| **Genetic Diversity** | ≥ 40 % unique tensor graphs in library | If the library's diversity drops below this KPI, Karn's Diversity Sampler will detect the change and increase its internal weighting for novelty to trigger "exotic blueprint" generation. |
| **Success Rate (10 generations)**| ≥ 80 % | Under-performing blueprints are flagged for archival. |
| **Latency Budget Breach** | \< 1 % of live inferences | The blueprint's WEP is automatically downgraded to Moderate. |
| **Energy‑Per‑Token Spike**| \< 5 % over rolling 24 h | The blueprint is flagged for manual review. |
| **Tamiyo Interest** | \< 0.7 favourability | Valued but middling performance blueprints are preserved. |
| **Training Impact** | ≥ 5 % improvement in reward | A blueprint sustaining this level of improvement can have its `conf_level` upgraded to High. |

---

## 7  Open Items (v1.0 todo)

Edge-benchmarks for FlashAttention Lite vs MQA. We need a clean apples-to-apples run on a Jetson Orin Nano, capturing perplexity, latency and joules-per-token, so Tamiyo has solid priors when choosing attention variants on low-power probes.

End-to-end JSON-schema validation in CI. A lightweight pytest plugin will parse every blueprint stub on each pull request, guaranteeing the metadata contract stays intact as the library grows.

Risk-index population from fresh Urabrask data. The current risk_level_idx numbers are blueprintsed from historical runs; they must be refreshed once the new canary system starts producing live failure-rate metrics.
